<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://pairlab.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://pairlab.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-23T15:18:03+00:00</updated><id>https://pairlab.github.io/feed.xml</id><title type="html">PAIR</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Value Gradient weighted Model-Based Reinforcement Learning</title><link href="https://pairlab.github.io/blog/2022/vagram-voelcker/" rel="alternate" type="text/html" title="Value Gradient weighted Model-Based Reinforcement Learning"/><published>2022-04-21T15:12:00+00:00</published><updated>2022-04-21T15:12:00+00:00</updated><id>https://pairlab.github.io/blog/2022/vagram-voelcker</id><content type="html" xml:base="https://pairlab.github.io/blog/2022/vagram-voelcker/"><![CDATA[<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> <p><a href="https://cvoelcker.de">Claas Voelcker</a>, <a href="https://victorliao.com/">Victor Liao</a>, <a href="https://animesh.garg.tech/">Animesh Garg</a>, <a href="https://sologen.net">Amir-massoud Farahmand</a><br/> <em><a href="https://iclr.cc/">ICLR 2022 (spotlight paper)</a></em><br/> <a href="https://openreview.net/pdf?id=4-D6CZkRXxI">paper</a>, <a href="https://github.com/pairlab/vagram">code</a></p> <p>With the rise of powerful and flexible function approximation, model-based reinforcement learning (MBRL) has gained a lot of traction in recent years. The core idea of MBRL is intuitive: (a) use the data obtained by online interaction of an agent with its environment, (b) build a surrogate model of this environment, and (c) use this model to improve the agent’s planning capabilities.</p> <table> <tbody> <tr> <td><img src="https://pairlab.github.io/assets/img/blog/vagram-apr22/dyna.png" alt="A DYNA algorithm visualized." width="100%"/></td> <td><em>A sketch of the DYNA algorithm. The model is used to generate additional data for the RL agent training.</em></td> </tr> </tbody> </table> <p>While intuitive, this approach can start to suffer once we obtain multimodal, high resolution sensory information. In these cases, the agent may be observing more of the world that it needs to for completing its task and creating a predictive model of the full environment may actually be harder than the task at hand itself!</p> <p>In most MBRL approaches, the environment model is obtained from the maximum likelihood objective, often via a reconstruction loss where the model tries to predict the next observation and agent will encounter from the previous observations and actions. However, if the observation contains many superfluous dimensions, the MLE objective is ineffective, as much of the model capacity is spent on approximating the full complexity of the observation space.</p> <p>Our key proposal is to regularize the model learning by the sensitivity of the value function for different inputs. Intuitively, if the value function is not influenced by changing the observation, the model is not required to be accurate. To highlight how we arrive at this solution, let’s look at the problem and previous proposals in more detail.</p> <h2 id="when-is-a-model-mismatched">When is a model mismatched?</h2> <table> <tbody> <tr> <td><img src="https://pairlab.github.io/assets/img/blog/vagram-apr22/model_mismatch.png" alt="A sketch of a driving car. Hikers by the side of the road are labeled important, a cloud is marked as unimportant." width="100%"/></td> <td><em>For a car driving task, it is important to differentiate important from unimportant environment features. While it is very important to predict the likelihood of pedestrians entering the road, the clouds in the sky are just distractions.</em></td> </tr> </tbody> </table> <p>This phenomenon has been termed the “objective mismatch”: the model does not know anything about the task the agent is trying to solve, no information from the task is fed back to the model learning, so the objectives of the agent (“obtaining high reward”) and the model (“achieving low reconstruction error”) do not necessarily align. When trying to solve this problem, we quickly run into a catch-22: one of the core assumptions underlying decision making is that we do not know how to solve the task, otherwise we would not be doing model learning in the first place. We need an accurate model of the environment before we can solve the task, so how can we feed task information back to the model before solving it?</p> <table> <tbody> <tr> <td><img src="https://pairlab.github.io/assets/img/blog/vagram-apr22/vaml_principle.png" alt="A sketch of the VAML principle. 4 models are shown, with different value functions." width="100%"/></td> <td><em>On the left side, the model predictions are correct. On the right hand side they are wrong. In the top right image, the model prediction does not lead to a change in the value function prediction, so no error is fed back. In the bottom right corner, the model error leads to a difference in value function, so the error is propagated to the RL algorithm.</em></td> </tr> </tbody> </table> <p>In their papers “Value-aware model learning” and “Iterative value-aware model learning”, Farahmand et al. present two potential solutions to the problem: by analyzing the way in which the model is used in a Dyna algorithm, they show that the model only influences the policy via its value function. This means that even if the model prediction is wrong, as long as the value function prediction aligns with the real environment, the RL agent is not impacted by the model error. Conversely, even if the model only makes a small error, if the value function is very sensitive to small changes in the state space, the resulting value prediction can be very wrong. Following from this observation, Farahmand et al. propose to replace the model learning loss with a loss that measures the difference in value function.</p> \[\mathcal{L}_V(\hat{p}, p, \mu) = \int \mu(s,a) \bigg|\overbrace{\int p(s'|s,a)V(s')\mathrm{d}s'}^{\text{environment value estimate}} - \overbrace{\int \hat{p}(s'|s,a) V(s') \mathrm{d}s'}^{\text{model value estimate}}\bigg|^2 \mathrm{d} (s,a)\] <h3 id="does-this-solve-the-objective-mismatch-problem">Does this solve the objective mismatch problem?</h3> <p>Although the theoretical underpinnings of the VAML approach are rigorous, when naively applying the algorithm in practice, two problems quickly become apparent.</p> <p>(1) In many RL environments, we cannot assume that the state space is fully explored in early iterations, which means that there are many possible states in which we do not have data to learn a value function. However, a function approximation will still assign a value to these points, interpolating from previously seen points in the training set, often resulting in nonsensical values. When the model predicts that a next state is in an unexplored region of the state space, the VAML loss will not penalize it for predicting a completely wrong state if the value functions align. In some cases it can even drive the prediction further into the unexplored regions, simply because it only seeks to find a local optimum of the value function prediction. When the value function is updated, the predictions in the regions of the state space that are not covered by data often change rapidly, which suddenly cause very large value function prediction errors when using model data.</p> <p>(2) The second problem is the smoothness of the value function and resulting VAML loss. In many common applications, the value function is not convex or smooth, exhibiting plateaus and ridges that make the VAML loss difficult to optimize. In the image below we show the value function of the Pendulum environment. The non-smooth nature of the function shows in the two sharp ridges. When non-smooth value functions are coupled with out-of-distribution value estimates (problem #1), this can result in massive gradient norms and subsequent gradient descent procedure using this estimate can diverge rapidly.</p> <h2 id="value-gradient-aware-model-learning">Value-gradient aware model learning</h2> <p>To solve the model mismatch problem without introducing new optimization challenges, we argue that a good model loss should have three properties:</p> <ul> <li>It should minimize the value prediction error under the model. This is the task awareness.</li> <li>It should not lead to models predicting next states outside of the data covered region. This ensures stability with function approximation.</li> <li>It should be reasonably smooth. This enables easier optimization.</li> </ul> <p>Our key insight is to include the value function into the loss as a measure of the sensitivity of model errors for different data points and observation dimensions. To estimate this, we compute a convex approximation of the value function around each data point by taking its first order Taylor approximation (squared).</p> <p>This gives us a measure of how sensitive the value function is to distortions in the state space. If the gradient of the value function is low in a specific dimension, the impact of model prediction errors will be relatively small. Conversely, in regions of high gradient, the value function prediction changes rapidly, so the model should measure these dimensions more carefully. In mathematical terms, the gradient allows us to augment the L2 loss in state space by a value function-dependent local regularization for each data point.</p> \[\hat{\mathcal{L}}_{\hat{V}}=\sum_{\{s_i,a_i,s'_i\}\in\mathcal{D}}{\left(V(s'_i) - \int \hat{P}_\theta(s'|s_i,a_i) (V(s'_i) + (\nabla_s V(s)|_{s'_i})^\intercal (s' - s'_i)) \mathrm{d}s'\right)^2}\] \[=\sum_{\{s_i,a_i,s'_i\}\in\mathcal{D}} {\left(\int \hat{P}_\theta(s'|s_i,a_i) \left((\nabla_s V(s)|_{s'_i})^\intercal(s' - s'_i) \right) \mathrm{d}s'\right)^2}\] <p>We call the loss function the Value Gradient weighted Model loss (VaGraM).</p> <table> <tbody> <tr> <td><img src="https://pairlab.github.io/assets/img/blog/vagram-apr22/all_losses.png" alt="The value function of a pendulum environment and all discussed loss functions (VAML, MSE, VaGrAM) are shown. VaGrAM has a paraboloid shape like MSE but follows the gradient of the value function." width="100%"/></td> <td><em>A visual comparison of all the discussed value functions.</em></td> </tr> </tbody> </table> <h2 id="does-it-actually-help-in-practice">Does it actually help in practice?</h2> <p>The theory underlying VaGraM and VAML tells us that a value-aligned loss function should matter most in scenarios where the model is insufficient to capture the full complexity of the environment, or in cases where there are irrelevant dimensions in the state space for the control task. To verify that VaGraM actually increases the performance of a state-of-the-art model based RL algorithm, we conducted two main experiments:</p> <p><em>(a) Does VaGraM help when the model doesn’t fit!</em></p> <p>We took the popular DM control environment Hopper and Model-based Policy Optimization (MBPO). We replaced the MLE loss in MBPO by VaGraM and gradually decreased the model size to limit its capacity. The performance of the maximum likelihood solution quickly deteriorated as the model grew smaller, while the performance of the VaGraM augmented version stayed stable.</p> <p><img src="https://pairlab.github.io/assets/img/blog/vagram-apr22/hopper_small_joint.png" alt="" width="100%"/></p> <p><em>(b) Does VaGraM help when the model is hard to fit due to distracting observations.</em></p> <p>We appended superfluous dimensions to the state space following an independent non-linear dynamical system. This proved to be a very challenging environment, and both MBPO and VaGraM dropped in performance quickly with an increasing number of distracting dimensions. Nonetheless, VaGraM managed to stabilize the Hopper and achieve some forward motion when faced with 15 distracting dimensions, while the MLE solution collapsed to the performance of a random policy.</p> <p><img src="https://pairlab.github.io/assets/img/blog/vagram-apr22/hopper_distraction_joint.png" alt="" width="100%"/></p> <p>In further experiments we found that VaGraM is able to perform on par with MBPO in all DM control environments, and even outperforms it on the Ant benchmark. We hypothesize that the Ant state space is not perfectly tuned for the control problem, which shows that task-aware losses can achieve better performance even in environments where we previously did not expect the state and observation space to contain superfluous information.</p> <p>If you want to use and expand VaGraM, the implementation of the core loss function is surprisingly easy and can be included into most deep learning frameworks with only one additional backpropagation pass through the value function network.</p> <p>Here is the loss function code using the jax library:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">vagram_loss</span><span class="p">(</span><span class="n">model_prediction</span><span class="p">,</span> <span class="n">environment_sample</span><span class="p">,</span> <span class="n">value_function</span><span class="p">):</span>
    <span class="n">err</span> <span class="o">=</span> <span class="n">model_prediction</span> <span class="o">-</span> <span class="n">environment_sample</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">value_and_grad</span><span class="p">(</span><span class="n">value_function</span><span class="p">)(</span><span class="n">environment_sample</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">square</span><span class="p">(</span><span class="n">grad</span> <span class="o">*</span> <span class="n">err</span><span class="p">).</span><span class="nf">sum</span><span class="p">()</span>
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[Value Gradient weighted Model-Based Reinforcement Learning presented at ICLR 2022]]></summary></entry><entry><title type="html">Remote Sim2Real for Dexterous In-Hand Manipulation</title><link href="https://pairlab.github.io/blog/2021/remote-sim2real-trifinger/" rel="alternate" type="text/html" title="Remote Sim2Real for Dexterous In-Hand Manipulation"/><published>2021-09-21T10:12:00+00:00</published><updated>2021-09-21T10:12:00+00:00</updated><id>https://pairlab.github.io/blog/2021/remote-sim2real-trifinger</id><content type="html" xml:base="https://pairlab.github.io/blog/2021/remote-sim2real-trifinger/"><![CDATA[<h2 id="transferring-dexterous-manipulation-from-gpu-simulation-to-a-remote-real-world-trifinger"><strong>Transferring Dexterous Manipulation from GPU Simulation to a Remote Real-World TriFinger</strong></h2> <p>Arthur Allshire, Mayank Mittal, Varun Lodaya, Viktor Makoviychuk, Denys Makoviichuk, Felix Widmaier, Manuel Wüthrich, Stefan Bauer, Ankur Handa, Animesh Garg.<br/> <a href="http://arxiv.org/abs/2108.09779">Paper</a>, <a href="https://s2r2-ig.github.io/">Project</a></p> <p><img src="https://pairlab.github.io/assets/img/blog/s2r2-sept21/s2r2-teaser.jpeg" alt="Teaser image for Remote Sim2Real." width="80%"/></p> <p><em>Figure: We have shown that large scale GPU-based simulation can be used to empower robot learning in simulation, and such solutions can be transferred to real robots without the need for physical access to the robots.</em></p> <h3 id="motivation">Motivation</h3> <h4 id="1-typical-robotics-solutions-take-weeks-if-not-months-to-develop-and-test">1. Typical robotics solutions take weeks, if not months, to develop and test</h4> <p>A critical question when designing a Machine Learning based solution is “what is the resource cost of developing this solution?”. There are typically many factors that go into answering this: time, developer skill, and computing resources. It’s rare that a researcher can maximize all of these aspects, hence optimizing the solution development process is critical. This problem is further aggravated in robotics since each task typically requires a completely unique solution that involves a non-trivial amount of hand-crafting from an expert!</p> <p>Dexterous multi-finger object manipulation has been one of the long-standing challenges in control and learning for robot manipulation [<a href="https://ieeexplore.ieee.org/document/844067">Okamura et al 2001.</a>, <a href="https://arxiv.org/abs/1806.00942">Sundaralingam et al. 2018</a>, <a href="https://ieeexplore.ieee.org/abstract/document/7487156">Kumar et al. 2016</a>, <a href="https://arxiv.org/abs/1810.06045">Zhu et al. 2018</a>]. While challenges in high-dimensional control for locomotion as well as image based object manipulation with simplified grippers have made remarkable progress in the last 5 years, Multi-finger Dexterous Manipulation remains a high-impact yet hard-to-crack problem. This challenge is due to a combination of issues:</p> <ul> <li>High-dimensional coordinated control,</li> <li>Inefficient simulation platforms,</li> <li>Uncertainty in observations and control in real-robot operation, and</li> <li>Lack of robust and cost-effective hardware platforms.</li> </ul> <p>These challenges coupled with lack of availability of large scale compute and robotic hardware has limited diversity among the teams attempting to address these problems.</p> <p><strong>Our goal in this effort is to present a path for democratization of robot learning and a viable solution through large scale simulation and robotics-as-a-service.</strong> We focus on 6-DoF object manipulation by using a dexterous multi-finger manipulator as a case study. We show how large-scale simulation done on a desktop grade GPU and cloud-based robotics can enable roboticists to perform research in robotic learning with modest resources.</p> <p>While a number of efforts in in-hand manipulation have attempted to build robust systems [<a href="https://ieeexplore.ieee.org/document/844067">Okamura et al 2001.</a>, <a href="https://arxiv.org/abs/1806.00942">Sundaralingam et al. 2018</a>, <a href="https://ieeexplore.ieee.org/abstract/document/7487156">Kumar et al. 2016</a>, <a href="https://arxiv.org/abs/1810.06045">Zhu et al. 2018</a>], one of the most impressive demonstrations came a few years ago from a team at OpenAI which built a system termed Dactyl [<a href="https://arxiv.org/pdf/1910.07113.pdf">OpenAI et al.</a>]. It was an impressive feat of engineering to achieve multi-object in-hand reposing with a Shadow Hand. It was remarkable not only for the final performance but also in the amount of compute and engineering effort to build this demo! As per public estimates, it used 13,000 years worth of computing and the hardware itself was costly and yet required repeated interventions! This immense resource requirement effectively prevented others from reproducing this result and as a result building on it.</p> <p>We show that our systems effort is a path to address this resource inequality, in the sense that a similar result can now be achieved in under a day using a single desktop-grade GPU and CPU.</p> <h4 id="2-the-complexity-of-standard-pose-representations-in-the-context-of-reinforcement-learning">2. The complexity of standard pose representations in the context of Reinforcement Learning</h4> <p>During initial experimentation, we followed previous works [<a href="https://openai.com/blog/learning-dexterity/">OpenAI et al, 2018</a>, <a href="http://proceedings.mlr.press/v87/liang18a/liang18a.pdf">Liang et al, 2018</a>] in providing our policy with observations based on a 3-D cartesian position plus a 4-Dimensional quaternion representation of pose to specify the current and target position of the cube, and reward based on L2 norm (position) and angular difference (orientation) between the desired and current pose of the cube. We found this approach to produce unstable reward curves which was good at optimising the position portion of the reward, even after adjusting relative weightings.</p> <p><img src="https://pairlab.github.io/assets/img/blog/s2r2-sept21/figure_4.png" alt="" width="50%"/></p> <p><strong>Training curves on the Trifinger manipulation task using a reward function similar to previous wokrs. The nature of the reward makes it very difficult for the policy to optimize, particularly achieving an orientation goal.</strong></p> <p>Prior work has shown the benefits of alternate representations of spatial rotation when using neural networks [<a href="https://arxiv.org/abs/1812.07035">Zhou et al, 2020</a>]. Furthermore, it has been shown that mixing losses this way can lead to collapsing towards only optimising a single objective [<a href="https://www.engraved.blog/why-machine-learning-algorithms-are-hard-to-tune/">Degrave et al. 2020</a>]. The chart implies a similar behaviour, where only position reward is being optimised for.</p> <p>Inspired by this, we searched for a representation of pose in SO(3) for our 6-DoF reposing problem, which would also naturally trade off position and rotation rewards in a way suited to optimisation via reinforcement learning.</p> <h4 id="3-closing-the-sim2real-gap-with-remote-robots">3. Closing the sim2real gap with remote robots</h4> <p>The problem of access to physical robotic resources was exacerbated by the COVID-19 pandemic. Those fortunate enough to previously have access to robots in their research groups found that the number of people with physical access to the robots was greatly decreased; those that relied on other institutions to provide the hardware were often alienated completely due to physical distancing restrictions.</p> <p>Our work demonstrated the feasibility of a Robotics as-a-Service (RaaS) approach in tandem with robot learning, where a small team of people (something about maintaining the robot) and a separate team of researchers were able to upload a trained policy remotely collect data for post-processing.</p> <p>While our team of researchers was primarily based in North America, the physical robot was located in Europe, and so for the duration of the project, our development team was never able to physically be in the same room as the robots we were working on. Remote acces meant we could not vary the task at hand to make it easier, and limited the kinds of iteration and experiments we could do. For example, a priori system identification was not possible as our policy ran on a randomly chosen robot in the entire farm.</p> <p>We found that despite the lack of physical access, we were able to produce a robust and working policy to solve the 6 Degree-of-Freedom reposing task through a combination of several techniques: Realistic GPU-accelerated simulation, model-free RL, domain randomisation, and task-appropriate representation of pose.</p> <h3 id="method-overview">Method Overview</h3> <p><img src="https://pairlab.github.io/assets/img/blog/s2r2-sept21/system.png" alt="" width="100%"/> <strong>Our system trains using the IsaacGym simulator on 16,384 environments in parallel on a single NVIDIA Tesla V100 or RTX 3090 GPU. Inference is then conducted remotely on a TriFinger robot located across the Atlantic in Germany using the uploaded actor weights. The infrastructure on which we perform Sim2Real transfer is provided courtesy of the organisers of the <a href="https://real-robot-challenge.com/">Real Robot Challenge</a>.</strong></p> <h4 id="1-collect-and-process-training-examples">1. Collect and process training examples</h4> <p>Using the IsaacGym simulator, we gather high-throughput experience (~100K samples/sec on an RTX 3090). The sample’s object pose and goal pose are to 8 key points of the object’s shape, and domain randomizations are applied to the observations, and environment parameters in order to simulate variations in the proprioceptive sensors of the real robots and cameras. These observations, along with some privileged state information from the simulator, were then used to train our policy.</p> <h4 id="2-train-policy">2. Train policy</h4> <p>Our policy is trained to maximize a custom reward using the Proximal Policy Optimization (PPO) algorithm (<a href="https://arxiv.org/abs/1707.06347">Schulman et al., 2017</a>). Our reward incentivizes the policy to balance the distance of the robot’s fingers from the object, the speed of movement, and the distance from the object to a specified goal position to solve the task efficiently, despite being a general formulation applicable broadly across in-hand manipulation applications. The policy outputs the torques for each of the robot’s motors, which are then passed back into the simulation environment.</p> <h4 id="3-transfer-policy-to-real-robot-and-run-inference">3. Transfer Policy to real robot and run inference</h4> <p>Once we have trained a policy, we upload it to the controller for the real robot. The cube is tracked on the system using 3 cameras. We combine proprioceptive information available from the system along with the converted keypoints representation to provide input to the policy. We repeat the camera-based cube-pose observations for subsequent rounds of policy evaluation to allow the policy to take advantage of the higher-frequency proprioceptive data available to the robot. The data collected from the system is then used to determine the success rate of the policy. The tracking system on the robot currently only supports cubes, however this could be extended in future to arbitrary objects.</p> <h3 id="results">Results</h3> <h4 id="1-the-key-points-representation-of-pose-greatly-improves-success-rate-and-convergence">1. The key points representation of pose greatly improves success rate and convergence</h4> <p><img src="https://pairlab.github.io/assets/img/blog/s2r2-sept21/four_setups.png" alt="" width="50%"/>.</p> <p><strong>Success Rate on the real robot plotted for different trained agents. O-PQ and O-KP stand for position+quaternion and keypoints observations respectively, and R-PQ and R-KP stand for linear+angular and keypoints based displacements respectively. Each mean made of N=40 trials and error bars calculated based on an 80% confidence interval.</strong></p> <p>We were able to demonstrate that the policies that used our keypoint representation in either the observation provided to the policy or in reward calculation achieved a higher success rate than using a position+quaternion representation, with the highest performance coming from the policies that used the alternate representation for both elements.</p> <p><img src="https://pairlab.github.io/assets/img/blog/s2r2-sept21/dr_no_dr.png" alt="" width="100%"/>.</p> <p><strong>Success Rate over the course of training without and with domain randomization. Each curve is the average of 5 seeds; the shaded areas show standard deviation. Note that training without DR is shown to 1B steps to verify performance; use of DR didn’t have a large impact on simulation success rates after initial training</strong></p> <p>We performed experiments to see how the use of keypoints impacted the speed and convergence level of our trained policies. As can be seen, using keypoints as part of the reward considerably sped up training, improved the eventual success rate, and reduced variance between trained policies. The magnitude of the difference was surprising to us given the simplicity and generality of using keypoints as part of reward.</p> <h4 id="2-the-trained-policies-can-be-deployed-straight-from-the-simulator-to-remote-real-robots">2. The trained policies can be deployed straight from the simulator to remote real robots</h4> <table> <thead> <tr> <th style="text-align: center">Dropping and Regrasping</th> <th style="text-align: center">Recovering from failure</th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><img src="https://pairlab.github.io/assets/img/blog/s2r2-sept21/drop_repose.gif" alt="" width="95%"/></td> <td style="text-align: center"><img src="https://pairlab.github.io/assets/img/blog/s2r2-sept21/recover.gif" alt="" width="95%"/></td> </tr> </tbody> </table> <p>The above demonstration displays an emergent behaviour that we’ve termed “dropping and regrasping”. In this maneuver, the robot learns to drop the cube when it is close to the correct position, re-grasp, and pick it back up. This enables the robot to get a stable grasp on the cube in the right position, which leads to more successful attempts. It’s worth noting that this video is in real-time, i.e., it is not sped up in any way.</p> <p>The robot also learns to use the motion of the cube to the correct location in the arena as an opportunity to simultaneously rotate it on the ground to achieve the correct grasp in challenging target locations far from the center of the fingers’ workspace.</p> <p>In this demonstration we also see how a poor grasp lead to the cube slipping. Without being explicitly trained to, the robot has learned how to recover from such falls.</p> <p>Our policy is also robust towards dropping - it is able to recover from a cube falling out of the hand and retrieve it from the ground.</p> <h4 id="3-robustness-to-physics-and-object-variations">3. Robustness to physics and object variations</h4> <p>We found that our policy was fairly robust to variations in environment parameters in simulation. For example, it gracefully handles scaling up and down of the cube by ranges far exceeding randomization:</p> <table> <thead> <tr> <th style="text-align: center">Scaled up cube</th> <th style="text-align: center">Scaled down cube</th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><img src="https://pairlab.github.io/assets/img/blog/s2r2-sept21/big_cube.gif" alt="" width="95%"/></td> <td style="text-align: center"><img src="https://pairlab.github.io/assets/img/blog/s2r2-sept21/small_cube.gif" alt="" width="95%"/></td> </tr> </tbody> </table> <p>Surprisingly, we found that our policies were able to generalise 0-shot to other objects, for example a cuboid or a sphere:</p> <table> <thead> <tr> <th style="text-align: center">Cuboid</th> <th style="text-align: center">Sphere</th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><img src="https://pairlab.github.io/assets/img/blog/s2r2-sept21/cuboid.gif" alt="" width="95%"/></td> <td style="text-align: center"><img src="https://pairlab.github.io/assets/img/blog/s2r2-sept21/sphere.gif" alt="" width="95%"/></td> </tr> </tbody> </table> <p>Note that generalisation in scale and object is taking place due to the policy’s own robustness: we do not give it any shape information. The keypoints remain in the same place as they would on a cube:</p> <p><img src="https://pairlab.github.io/assets/img/blog/s2r2-sept21/keypoints_cube.gif" alt="" width="60%"/></p> <p><em>An example of a successful completion of the manipulation task with the keypoints visualized.</em></p> <h3 id="takeaways-and-potentials">Takeaways and Potentials</h3> <p>Our method shows a viable path for robot learning through large scale GPU-based simulation. We show how it is possible to train a policy using moderate levels of computational resources (desktop-level compute) and transfer to a remote robot. We also show that these policies are robust to a variety of changes in the environment and the object being manipulated. We hope our work can serve as a platform for researchers going forward.</p> <h3 id="paper-covered-in-this-post">Paper covered in this post</h3> <p><a href="https://arxiv.org/abs/2108.09779"><strong>Transferring Dexterous Manipulation from GPU Simulation to a Remote Real-World TriFinger</strong></a>.<br/> <a href="https://allshire.org/">Arthur Allshire</a>, <a href="https://mayankm96.github.io/">Mayank Mittal</a>, <a href="https://ca.linkedin.com/in/varun-lodaya">Varun Lodaya</a>, <a href="https://www.linkedin.com/in/makoviychuk-viktor-9199988">Viktor Makoviychuk</a>, <a href="https://www.linkedin.com/in/denys-makoviichuk-2219a72b">Denys Makoviichuk</a>, <a href="https://scholar.google.de/citations?user=mmfbJaEAAAAJ&amp;hl=en">Felix Widmaier</a>, <a href="https://scholar.google.de/citations?user=7EWrVYIAAAAJ&amp;hl=en">Manuel Wuthrich</a>, <a href="https://www.is.mpg.de/~sbauer">Stefan Bauer</a>, <a href="https://ankurhanda.github.io/">Ankur Handa</a>, <a href="http://animesh.garg.tech/">Animesh Garg</a>.<br/> Under Review</p> <p><strong>Acknowledgement:</strong> This work was led by University of Toronto in collaboration with Nvidia, Vector Institute, MPI, ETH and Snap. We would like to thank Vector Institute for computing support, as well as the CIFAR AI Chair for research support to Animesh Garg.</p> ]]></content><author><name></name></author><summary type="html"><![CDATA[Transferring Dexterous Manipulation from GPU Simulation to a Remote Real-World TriFinger]]></summary></entry><entry><title type="html">Learning Tool Affordances without Labels</title><link href="https://pairlab.github.io/blog/2021/giftturpin/" rel="alternate" type="text/html" title="Learning Tool Affordances without Labels"/><published>2021-06-15T15:12:00+00:00</published><updated>2021-06-15T15:12:00+00:00</updated><id>https://pairlab.github.io/blog/2021/giftturpin</id><content type="html" xml:base="https://pairlab.github.io/blog/2021/giftturpin/"><![CDATA[<h2 id="generalizable-interaction-aware-functional-tool-affordances-without-labels"><strong>Generalizable Interaction-aware Functional Tool Affordances without Labels</strong></h2> <p><a href="http://www.cs.toronto.edu/~dylanturpin/">Dylan Turpin</a>, <a href="https://www.linkedin.com/in/liquan-wang-a37634196/?originalSubdomain=ca">Liquan Wang</a>, <a href="https://tsogkas.github.io/">Stavros Tsogkas</a>, <a href="https://www.cs.toronto.edu/~sven/">Sven Dickinson</a>, <a href="https://animesh.garg.tech/">Animesh Garg</a><br/> <em><a href="http://www.roboticsproceedings.org/rss17/p060.html">Robotics Systems &amp; Science</a>, 2021.</em><br/> <a href="https://arxiv.org/abs/2106.14973">paper</a>, <a href="https://youtu.be/7N1XiIzu9v4">video</a></p> <p><img src="https://pairlab.github.io/assets/img/blog/gift-jul21/gift-teaser.svg" alt="Teaser image for GIFT. Discover tool affordances by interacting with procedurally-generated tools across three manipulation tasks: hooking, reaching and hammering. Train an affordance model to detect sparse keypoints representing tool geometry and predict distributions over pairs of keypoints to grasp and interact with for each task by learning from the contact data of sampled trajectories. Affordance predictions from RGBD observations of unknown objects match expected task semantics across hooking, reaching and hammering and are similar to those of a human labeller, e.g. for hammering." width="120%"/></p> <p><em>Figure 1: Rather than relying on human labels, the GIFT framework discovers affordances from goal-directed interaction with a set of procedurally-generated tools.</em></p> <h3 id="motivation">Motivation</h3> <p><strong>1. We should represent tools by what we can do with them.</strong></p> <p>When it comes to tools, ‘‘What can I do with this?’’ is the key question and there usually isn’t just one answer. There are many basic ways of using a hammer (to strike, to pry, to reach) each of which contains many finer-grained possibilities (strike with a high grip for precision, or a low grip for power).</p> <p>We learn a representation that captures these possibilities. Specifically, we represent an action possibility (i.e., an affordance) as a tuple (task ID, grasp keypoint, interaction keypoint).</p> <p>We build on a line of work that investigates behaviour-grounded object representations, especially KETO (<a href="https://arxiv.org/abs/1910.11977"><em>Fang et al. 2019</em></a>) and kPAM (<a href="https://arxiv.org/abs/1903.06684"><em>Manuelli et al. 2019</em></a>). In contrast to these, our learned representations do not rely on human labels (as in kPAM) or a predefined manipulation strategy (as in KETO).</p> <p><strong>2. Behaviour-grounded predictions are testable predictions.</strong></p> <p>Because our predicted tool representations are <em>behaviour-grounded</em> (i.e., they correspond to possible actions) we can test them against reality by executing the corresponding actions and checking the result. This ‘‘predict, act, check’’ routine gives us a <em>self-supervised</em> training loop that does not rely on human labels.</p> <p>To close the loop, we need a way of translating predicted representations into executable motions, which is a challenge, because the space of possible actions is large. Prior works rely on workarounds like additional human supervision or constraining the action space, but these simplifications come with serious drawbacks.</p> <p><em>Human supervision</em> (e.g., with keypoint labels) is expensive and introduces human bias. We want our representations to be discovered only from the constraints of the manipulation tasks.</p> <p><em>Constraining the action space</em> (e.g., to pre-defined motion primitives) means some action possibilities will never be explored.</p> <p><strong>3. Constraining behaviour limits affordance discovery.</strong></p> <p>If we constrain behaviour by limiting the action space or using a pre-defined manipulation strategy, we will never discover affordances corresponding to excluded behaviours.</p> <p>We generate our trajectories with a simple sampling-based motion planner that is conditioned on the predicted keypoints through a reward function with one term encoding task success and another encouraging use of the selected keypoints.</p> <p>Actions are sampled from the full action space, so motion generation is free to use tools in unexpected ways, discovering new possibilities that could not have been explored if we were tied to a limited set of motion primitivies or a pre-defined manipulation strategy.</p> <h3 id="method-overview">Method overview</h3> <p><img src="https://pairlab.github.io/assets/img/blog/gift-jul21/gift-pipeline.svg" alt="The training pipeline." width="120%"/></p> <p><em>Figure 2: The training pipeline. Our framework learns affordance models for hooking, reaching and hammering by interacting with a set of tools. All three models share a task-independent keypoint detector, which takes an RGBD image of a tool and predicts a set of keypoints representing a tool’s geometry and providing possible choices of grasp and interaction regions. The task-conditional portion of each model, which is trained on the outcome of trajectories collected from motion planning, selects two keypoints which become our functional tool representation.</em></p> <p><strong>Collect experience in the full action space.</strong></p> <p>We begin by generating a set of tools as concatenations of pairs of convex meshes into T, X and L shapes. We sample a tool from our training set, place it on a table and capture an RGBD observation of the tool from above.</p> <p>From this observation, our SparseKP network infers a set of keypoints representing the tool’s geometry and providing possible choices of regions to grasp and interact with. This module is pre-trained using unsupervised keypoint losses and is shared across tasks.</p> <p>A grasp keypoint and provisional interaction keypoint are uniformly sampled from the sparse ones. We take a crop of the RGBD observation around the grasp keypoint and pass it to a grasping network to infer a nearby stable grasp. This grasp is executed and we use MPPI to generate the rest of the trajectory. MPPI iteratively samples action sequences from the full action space and takes their average weighted by reward.</p> <p>Part of this reward encodes task success and part encourages use of the provisional interaction keypoint. In this way we are able to sample from the full action space while still conditioning on the sparse selection of keypoints on the tool’s surface.</p> <p><strong>Extract training examples from trajectory contact data.</strong></p> <p>Once we have a trajectory, we extract a training example. This consists of a reward, grasp KP, interaction KP and the full set of sparse keypoints. We replace the provisional choice of interaction KP with the keypoint closest to the actual first contact between the tool and the target object.</p> <p>The provisional interaction KP is sampled, and conditioned on, in order to encourage exploration of different manipulation strategies. But in the end, we want to use whichever interaction keypoint works best for the task.</p> <p><strong>Train keypoint selection to maximize task reward.</strong></p> <p>We sample a training example and build a graph out of its sparse keypoints. This graph is fed to a task-specific GNN, which predicts a distribution over pairs of keypoint indices (i.e., the joint distribution over grasp and interaction keypoints). Finally, we update the GNN weights using REINFORCE.</p> <p>At test time, we sample a tool from the holdout test set. The grasp and interaction keypoints are selected based on the predicted distribution, rather than uniformly, and we enforce that the selected interaction point be used, rather than treating it as provisional.</p> <h3 id="results">Results</h3> <p>So, how well does it work in practice?</p> <p>Quantitatively GIFT beats baseline methods on all three tasks and qualitatively the choices of grasp and interaction points usually match task semantics and agree with the choices of a human oracle.</p> <p><img src="https://pairlab.github.io/assets/img/blog/gift-jul21/gift-table.svg" alt="Quantitative results." width="70%"/></p> <p><em>Table 1: GIFT outperforms baselines on all tasks and matches a human oracle on two of three tasks using novel tools. Reward is normalized with respect to the human oracle.</em></p> <div style="width: 100%; height: 0px; position: relative; padding-bottom: 56.250%;"><iframe src="https://streamable.com/e/6l3jwp" frameborder="0" width="100%" height="100%" allowfullscreen="" style="width: 100%; height: 100%; position: absolute;"></iframe></div> <p><em>Video 1: Hammering a peg with tools from the holdout set. Low grasp points increase leverage (providing greater strike point velocity and peg impulse for a given joint velocity). Strike points on the hard metallic head allow accumulated kinetic energy to be rapidly transferred to the peg for maximum impulse.</em></p> <div style="width: 100%; height: 0px; position: relative; padding-bottom: 56.250%;"><iframe src="https://streamable.com/e/sehup0" frameborder="0" width="100%" height="100%" allowfullscreen="" style="width: 100%; height: 100%; position: absolute;"></iframe></div> <p><em>Video 2: Hooking a thermos with tools from the holdout set. Our original intention was to constrain the task such that only the right angle between head and handle could be used to hook. In fact, the motion planner finds other solutions.</em></p> <div style="width: 100%; height: 0px; position: relative; padding-bottom: 56.250%;"><iframe src="https://streamable.com/e/ciwknw" frameborder="0" width="100%" height="100%" allowfullscreen="" style="width: 100%; height: 100%; position: absolute;"></iframe></div> <p><em>Video 3: Constrained reaching with tools from the holdout set. To manipulate an object on the other side of a wall, the tool must fit through the hole and reach the target object.</em></p> <h3 id="future-directions">Future directions</h3> <p>By grounding our affordance representation in contact data and sampling trajectories from the full action space, we are able to discover unbiased affordances for each task without human labels. There are however important limitations to our method that we think future work could address.</p> <p>Our sparse keypoints are stored as raw locations, without any additional property encodings. A natural extension to our method would encode additional local information at each keypoint. This could capture local geometric or material properties and allow for more robust reasoning about the relationship between materials, fine-grained geometry and task requirements.</p> <p>Our motion sampling routine depends on access to simulatable dynamics. This makes it non-trivial to transfer our results to real robots. We plan to experiment with recovering 3D models from visual observations, so we can leverage our learned representation to plan in simulator and execute on a real robot.</p> <p>For details on the background, implementation and results read the full paper <a href="https://arxiv.org/abs/2106.14973">here</a> and watch the <a href="https://streamable.com/eylzdj">presentation</a>.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[GIFT: Generalizable Interaction-aware Functional Tool representations (at RSS 2021)]]></summary></entry></feed>